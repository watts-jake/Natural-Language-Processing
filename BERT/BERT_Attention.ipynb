{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_Attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Biweekly Report 3\n",
        "\n",
        "# Visualizing BERT Attention\n",
        "\n",
        "## Jake Watts\n",
        "\n"
      ],
      "metadata": {
        "id": "-GlgO5WBOYeF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section of the report, I visualize the attention from the trained BERT model used in the BERT_Tuning notebook on the MRPC data. I first visualize the attention between two sentences that are not semantically equivalent. I then visualize the attention between two sentences that are semantically equivalent. My goal in visualizing these pairs is to get a better understanding of BERT attention and to see if there is a difference in attention between equivalent and non-equivalent sentences.\n",
        "\n",
        "Note: Since the visualizations are interactive they can't be viewed within GitHub and have to be run within a Jupyter Notebook.\n",
        "\n",
        "Sources:\n",
        "\n",
        "https://github.com/jessevig/bertviz#setting-default-layer-head-s\n",
        "\n",
        "https://arxiv.org/abs/1810.04805"
      ],
      "metadata": {
        "id": "Yi1N-2S2As5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bertviz"
      ],
      "metadata": {
        "id": "aR07__FyOf8a",
        "outputId": "9450ac0c-ac3f-48ab-a86e-7adc0a1ad1ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bertviz\n",
            "  Downloading bertviz-1.3.0-py3-none-any.whl (155 kB)\n",
            "\u001b[K     |████████████████████████████████| 155 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting transformers>=2.0\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 62.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0 in /usr/local/lib/python3.7/dist-packages (from bertviz) (1.10.0+cu111)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from bertviz) (4.62.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from bertviz) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 47.0 MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "  Downloading boto3-1.21.4-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 21.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bertviz) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0->bertviz) (3.10.0.2)\n",
            "Collecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 48.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=2.0->bertviz) (4.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=2.0->bertviz) (3.6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 53.6 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 52.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.0->bertviz) (1.21.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.0->bertviz) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=2.0->bertviz) (3.0.7)\n",
            "Collecting botocore<1.25.0,>=1.24.4\n",
            "  Downloading botocore-1.24.4-py3-none-any.whl (8.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.5 MB 40.6 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.1-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.9 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.4->boto3->bertviz) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 59.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.4->boto3->bertviz) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=2.0->bertviz) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->bertviz) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bertviz) (2.10)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 56.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bertviz) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=2.0->bertviz) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=2.0->bertviz) (1.1.0)\n",
            "Installing collected packages: urllib3, jmespath, pyyaml, botocore, tokenizers, sacremoses, s3transfer, huggingface-hub, transformers, sentencepiece, boto3, bertviz\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed bertviz-1.3.0 boto3-1.21.4 botocore-1.24.4 huggingface-hub-0.4.0 jmespath-0.10.0 pyyaml-6.0 s3transfer-0.5.1 sacremoses-0.0.47 sentencepiece-0.1.96 tokenizers-0.11.5 transformers-4.16.2 urllib3-1.25.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I load the pre-trained BERT model. Sentence A and Sentence B are sentenced from the MRPC that are not equivalent. They are converted into tokens and then used to calculate attention."
      ],
      "metadata": {
        "id": "e5QQLQFXEf5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and retrieve attention weights\n",
        "\n",
        "from bertviz import head_view, model_view\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "model_version = 'bert-base-uncased'\n",
        "model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_version)\n",
        "sentence_a = \"The identical rovers will act as robotic geologists , searching for evidence of past water .\"\n",
        "sentence_b = \"The rovers act as robotic geologists , moving on six wheels .\"\n",
        "inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt')\n",
        "input_ids = inputs['input_ids']\n",
        "token_type_ids = inputs['token_type_ids']\n",
        "attention = model(input_ids, token_type_ids=token_type_ids)[-1]\n",
        "sentence_b_start = token_type_ids[0].tolist().index(1)\n",
        "input_id_list = input_ids[0].tolist() # Batch index 0\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_id_list) "
      ],
      "metadata": {
        "id": "7Hq61CFdACS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sentence A:\", sentence_a)\n",
        "print(\"Sentence B:\", sentence_b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqGo3mrNJ1OL",
        "outputId": "de579b4b-e9c3-4d61-f9af-d083305eb3f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence A: The 2002 second quarter results don 't include figures from our friends at Compaq .\n",
            "Sentence B: The year-ago numbers do not include figures from Compaq Computer .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the visualization for the attention, set to be displayed in the dark mode of colab. The grid is 12x12 and displays the attention for each attention head under each layer. To view only the attention between sentences A and B, I chose \"Sentence A -> Sentence B\" under the attention dropdown at the top. To view more details about a specific layer and attention head you can click on a square in the grid to see the words and attention.\n",
        "From the visualization, we can see that the attention layers show stronger connections as the layer number increases. It also appears that the words in sentence A are connected to the SEP token in a sizeable number of grids. Some grids also display a different pattern in which there are parallel lines in the top half of the grid. Clicking on those grids reveals that they connect the words that both sentences share, which mostly occur in the first half of the sentences.\n"
      ],
      "metadata": {
        "id": "uGZwj90HIH24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_view(attention, tokens, sentence_b_start, display_mode=\"dark\")"
      ],
      "metadata": {
        "id": "FHiBHbogPYwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's look at sentences that are semantically equivalent. Sentence A and B are printed below."
      ],
      "metadata": {
        "id": "RygUpvsdMyP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_version = 'bert-base-uncased'\n",
        "model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_version)\n",
        "sentence_a = \"The 2002 second quarter results don 't include figures from our friends at Compaq .\"\n",
        "sentence_b = \"The year-ago numbers do not include figures from Compaq Computer .\"\n",
        "inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt')\n",
        "input_ids = inputs['input_ids']\n",
        "token_type_ids = inputs['token_type_ids']\n",
        "attention = model(input_ids, token_type_ids=token_type_ids)[-1]\n",
        "sentence_b_start = token_type_ids[0].tolist().index(1)\n",
        "input_id_list = input_ids[0].tolist() # Batch index 0\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_id_list) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpK0wWnkADUS",
        "outputId": "d6ab6568-f6ca-464d-d943-ec4461634b19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sentence A:\", sentence_a)\n",
        "print(\"Sentence B:\", sentence_b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hT85APT7MvTI",
        "outputId": "74f5f77c-1534-4db0-891b-cdc943d8ec1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence A: The 2002 second quarter results don 't include figures from our friends at Compaq .\n",
            "Sentence B: The year-ago numbers do not include figures from Compaq Computer .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at Sentence A -> Sentence B attention shows a similar pattern in which some grids show connections to the SEP token and some show attention between words with equivalent meaning between the sentences. For example layer 4, head 3 shows an attention mapping between equivalent words across the two sentences. Since these sentences are semantically equivalent, there are more these connections in this visualization than the previous one."
      ],
      "metadata": {
        "id": "3eaHdAGKNGbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_view(attention, tokens, sentence_b_start, display_mode=\"dark\")"
      ],
      "metadata": {
        "id": "V-TlVzZiPbDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "Looking at attention visualizing is helpful in exploring and understanding the BERT model. From the connection visualizations, it is also easy to see some differences between the sentence pairs that are and are not semantically equivalent. The trained model has a grasp of the english language and it makes sense that this model can be quickly fine-tuned for evaluating sentence equivalence."
      ],
      "metadata": {
        "id": "I3iQSs2dAfj-"
      }
    }
  ]
}